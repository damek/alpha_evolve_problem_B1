{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B1: First autocorrelation inequality\n",
    "For any function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$, define the *autoconvolution* of $f$, written $f*f$, as\n",
    "$$f*f (t) := \\int_\\mathbb{R} f(t-x) f(x)\\ dx.$$\n",
    "\n",
    "Let $C_1$ denote the largest constant for which one has\n",
    "\\begin{equation}\n",
    " \\max_{-1/2 \\leq t \\leq 1/2} f*f(t) \\geq C_1 \\left(\\int_{-1/4}^{1/4} f(x)\\ dx\\right)^2\n",
    "\\end{equation}\n",
    "for all non-negative $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.  This problem arises in additive combinatorics, relating to the size of Sidon sets.  It is currently known that\n",
    "$$ 1.28 \\leq C_1 \\leq 1.5098$$\n",
    "with the lower bound proven by [Cloninger and Steinerberger (2017)](https://www.ams.org/journals/proc/2017-145-08/S0002-9939-2017-13690-9/S0002-9939-2017-13690-9.pdf) and the upper bound achieved by [Matolcsi and Vinuesa (2010)](https://www.sciencedirect.com/science/article/pii/S0022247X10006001) via a step function construction. AlphaEvolve found a step function with 600 equally-spaced intervals on $[-1/4,1/4]$ that gives a better upper bound of $C_1 \\leq 1.5053$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the step function, the integral, and the autoconvolution\n",
    "\n",
    "For step func. \n",
    "$$f(x) = \\sum_{i=0}^{P-1} h_i \\mathbf{1}_{[x_i, x_{i+1})}(x)$$  \n",
    "where\n",
    "$$ x_i = -\\frac{1}{4} + \\frac{i}{2P}, \\text{ and heights } h_i \\ge 0:$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "% Integral\n",
    "\\text{Integral:}\\quad \\int_{-1/4}^{1/4} f(x) \\,dx &= \\frac{1}{2P} \\sum_{i=0}^{P-1} h_i\n",
    "\\\\[2ex]\n",
    "% Autoconvolution\n",
    "\\text{Autoconv. knots:}\\quad (f*f)(t_m) &= \n",
    "\\begin{cases} \n",
    "0 & \\text{if } m=0 \\text{ or } m=2P \\\\\n",
    "\\frac{1}{2P} \\sum_{k=\\max(0, m-P)}^{\\min(P-1, m-1)} h_k h_{m-1-k} & \\text{if } 1 \\le m \\le 2P-1\n",
    "\\end{cases}\n",
    "\\\\[0.5ex]\n",
    "&\\quad \\text{where } t_m = -\\frac{1}{2} + \\frac{m}{2P} \\quad (m=0, \\dots, 2P).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of the optimization problem\n",
    "\n",
    "Let $f_h$ denote the step function with heights $h$. We're trying to optimize the function:\n",
    "\\begin{align*}\n",
    "\\mathrm{minimize} &\\; L(h) := \\max_{-1/2\\leq t \\leq 1/2} (f_h\\ast f_h)(t) \\\\\n",
    "\\text{subject to } &\\; \\int_{-1/4}^{1/4} f_h(t) dt = \\frac{\\mathbf{1}^T h}{2P} = 1 \\\\\n",
    "&\\; h \\geq 0.\n",
    "\\end{align*}\n",
    "This is a nonsmooth qudaratic optimization problem over a simplex.\n",
    "\n",
    "Note that this is not convex: set $P=2$ and $h_0+h_1=2P=4$. The objective is\n",
    "$$\n",
    "\\frac{1}{2P}\\max(h_0^2, 2h_0h_1, h_1^2)\n",
    "$$\n",
    "We can parameterize the feasible region by $h_0 \\in [0,4]$. Then there is a region where $2h_0h_1 = 2h_0(4-h_0)$ attains the max and the function becomes concave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the step function and the autoconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script assumes 'compute_integral_of_step_function' and 'compute_autoconvolution_values'\n",
    "# (from your previous code block) are already defined and executable in the notebook.\n",
    "from utils import *\n",
    "\n",
    "# --- main script logic ---\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "\n",
    "# 1. sample random heights (non-negative)\n",
    "# using float64 for precision, requires_grad=False for plotting/evaluation only\n",
    "height_values = torch.rand(P_val, dtype=torch.float64) \n",
    "# for more structured/sparse functions, you could try:\n",
    "# height_values = torch.abs(torch.randn(P_val, dtype=torch.float64)) * (torch.rand(P_val, dtype=torch.float64) > 0.7).double()\n",
    "\n",
    "\n",
    "# 2. compute integral of f(x)\n",
    "integral_value = compute_integral_of_step_function(height_values, f_delta_x)\n",
    "\n",
    "# 3. compute autoconvolution (f*f)(t) knot values\n",
    "# (2*P_val + 1) values for t_m from 2*f_x_min to 2*f_x_max\n",
    "autoconv_knot_vals = compute_autoconvolution_values(height_values, f_delta_x, P_val)\n",
    "max_autoconv_value = torch.max(autoconv_knot_vals)\n",
    "\n",
    "# 4. calculate the ratio for C1 estimate\n",
    "# (max f*f(t)) / (integral f(x) dx)^2\n",
    "if integral_value.item() == 0:\n",
    "    c1_ratio = float('inf') if max_autoconv_value.item() > 0 else 0.0 # handle division by zero\n",
    "else:\n",
    "    c1_ratio = max_autoconv_value / (integral_value**2)\n",
    "\n",
    "print(f\"--- Function Details (P={P_val}) ---\")\n",
    "# print(f\"Heights (h_i, first 10): {height_values.numpy()[:10]}\") # uncomment if you want to see heights\n",
    "print(f\"Integral of f(x): {integral_value.item():.6f}\")\n",
    "print(f\"Max value of autoconvolution (f*f)(t): {max_autoconv_value.item():.6f}\")\n",
    "print(f\"Ratio max(f*f) / (integral(f))^2: {c1_ratio.item():.6f}\")\n",
    "\n",
    "\n",
    "# 5. plotting\n",
    "# plot f(x)\n",
    "plot_rendered_step_function(height_values.numpy(), f_interval, title=f\"Random Step Function f(x) (P={P_val})\")\n",
    "\n",
    "# plot f*f(t)\n",
    "# autoconvolution is defined on [2*f_x_min, 2*f_x_max]\n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals.numpy(), title=f\"Autoconvolution f*f(t) (P={P_val})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Google's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "h_params_google = return_height_params_google_simplex()\n",
    "autoconv_knot_vals_google = compute_autoconvolution_values(h_params_google, f_delta_x, P_val)\n",
    "max_autoconv_value_google = torch.max(autoconv_knot_vals_google)\n",
    "\n",
    "plot_rendered_step_function(h_params_google.detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google.detach().numpy(), title=f\"Google'sAutoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: (Projected) Polyak subgradient method\n",
    "\n",
    "The algorithm \n",
    "$$\n",
    "h_+ = \\mathrm{proj}_{\\Delta_{2P}}\\left(h - \\frac{L(h) - L^*}{\\| \\nabla L(h) \\|^2} \\nabla L(h)\\right)\n",
    "$$\n",
    "where $L^*$ is the target loss.\n",
    "\n",
    "There is a subtlety here: we don't know what the optimal value is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak with Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "from utils import *\n",
    "# Initialize random height parameters\n",
    "h_params_polyak = torch.rand(P_val, dtype=torch.float64)\n",
    "h_params_polyak.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak.data = projection(h_params_polyak.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "# Setting up history\n",
    "if 'history_polyak_constrained_random' not in locals():\n",
    "    history_polyak_constrained_random = {}\n",
    "else:\n",
    "    h_params_polyak.data = history_polyak_constrained_random['best_h_params'].data.clone()\n",
    "\n",
    "# Setting up parameters for Polyak\n",
    "max_iter = 1000\n",
    "target_loss = 1.505\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_constrained_random = polyak_subgradient_method(h_params_polyak,loss_fn, max_iter, target_loss, print_every, history_polyak_constrained_random, projection)\n",
    "print(\"Best loss found:\", history_polyak_constrained_random['min_loss_found'])\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_constrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoconv_knot_vals_polyak = compute_autoconvolution_values(history_polyak_constrained_random['best_h_params'], f_delta_x, P_val)\n",
    "max_autoconv_value_polyak = torch.max(autoconv_knot_vals_polyak)\n",
    "\n",
    "plot_rendered_step_function(history_polyak_constrained_random['best_h_params'].detach().numpy(), f_interval, title=f\"Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_polyak.detach().numpy(), title=f\"Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak started at the Google solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "from utils import *\n",
    "h_params_polyak_google = h_params_google.data.detach().clone() # initialize variable at google's solution.\n",
    "h_params_polyak_google.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak_google.data = projection(h_params_polyak_google.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "# Setting up history\n",
    "if 'history_polyak_constrained_google' not in locals():\n",
    "    history_polyak_constrained_google = {}\n",
    "else:\n",
    "    h_params_polyak_google.data = history_polyak_constrained_google['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 1000\n",
    "target_loss_google = 1.505292\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_constrained_google = polyak_subgradient_method(h_params_polyak_google, loss_fn, max_iter, target_loss_google, print_every, history_polyak_constrained_google, projection)\n",
    "\n",
    "print(\"Best loss found (Google solution initialization):\", history_polyak_constrained_google['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_constrained_google['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method (Google solution initialization): Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "autoconv_knot_vals_google_polyak = compute_autoconvolution_values(history_polyak_constrained_google['best_h_params'], f_delta_x, P_val)\n",
    "max_autoconv_value_google_polyak = torch.max(autoconv_knot_vals_google_polyak)\n",
    "\n",
    "plot_rendered_step_function(history_polyak_constrained_google['best_h_params'].detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google_polyak.detach().numpy(), title=f\"Google's Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "\n",
    "print(\"best_height_params_polyak_google\", history_polyak_constrained_google['best_h_params'])\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the loss function to remove constraints\n",
    "\n",
    "We're going to precompose the loss function with a softmax. This implicitly constrains the functions to the simplex. I.e., we will optimize\n",
    "\n",
    "$$\n",
    "L(2P\\mathrm{softmax}(w))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyak on unconstrained (softmax) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "P_val = 600\n",
    "h_params_polyak_unconstrained = torch.rand(P_val, dtype=torch.float64)\n",
    "h_params_polyak_unconstrained.requires_grad = True\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "\n",
    "if 'history_polyak_unconstrained_random' not in locals():\n",
    "    history_polyak_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_polyak.data = history_polyak_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 100\n",
    "target_loss = 1.52\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_unconstrained_random = polyak_subgradient_method(h_params_polyak_unconstrained, loss_f_softmax, max_iter, target_loss, print_every, history_polyak_unconstrained_random)\n",
    "\n",
    "print(\"Best loss found:\", history_polyak_unconstrained_random['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_polyak_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS on unconstrained loss (with weak wolfe line search)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "from utils import *\n",
    "# h_params_bfgs_unconstrained = (torch.randn(P_val).abs() * 0.01 + 0.01).log()\n",
    "h_params_bfgs_unconstrained = torch.randn(P_val)\n",
    "# h_params_bfgs_unconstrained = h_params_google # Start from google's solution if you want.\n",
    "# h_params_bfgs_unconstrained.data = torch.log(h_params_bfgs_unconstrained.data)\n",
    "h_params_bfgs_unconstrained.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "if 'history_bfgs_unconstrained_random' not in locals():\n",
    "    history_bfgs_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_bfgs_unconstrained.data = history_bfgs_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 20\n",
    "bfgs_params = {}\n",
    "bfgs_params['lr'] = 1\n",
    "bfgs_params['history_size'] = 2000\n",
    "bfgs_params['ls_params'] = {'c1': 0.0001, 'c2': 0.9}\n",
    "# bfgs_params['line_search_fn'] = None # no line search function. Will likely blow up.\n",
    "bfgs_params['line_search_fn'] = 'weak_wolfe' # good for later iterations\n",
    "print_every = 100\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_softmax, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "# loss_f_normalized = scale_invariant_h_squared_loss(P_val=600)\n",
    "# history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_normalized, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "# loss_f_normalized_allow_negative = scale_invariant_h_squared_loss(P_val=600)\n",
    "# history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_normalized_allow_negative, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "\n",
    "\n",
    "print(\"Best loss found:\", history_bfgs_unconstrained_random['min_loss_found'])\n",
    "\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_bfgs_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"BFGS method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_bfgs_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"BFGS method: Gradient Norm History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTD on unconstrained loss \n",
    "\n",
    "From https://github.com/COR-OPT/ntd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "from utils import *\n",
    "h_params_ntd_unconstrained = torch.rand(P_val, dtype=torch.float64)\n",
    "# h_params_ntd_unconstrained = h_params_google # Start from google's solution if you want.\n",
    "h_params_ntd_unconstrained.data = torch.log(h_params_ntd_unconstrained.data)\n",
    "h_params_ntd_unconstrained.requires_grad = True\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "loss_f_normalized = scale_invariant_h_squared_loss(P_val=600)\n",
    "\n",
    "if 'history_ntd_unconstrained_random' not in locals():\n",
    "    history_ntd_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_ntd_unconstrained.data = history_ntd_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 1\n",
    "ntd_params = {}\n",
    "ntd_params['adaptive_grid_size'] = True\n",
    "ntd_params['verbose'] = False\n",
    "ntd_params['max_grid_size'] = 20 # 50 \n",
    "# ntd_params['opt_f'] = 1.5052\n",
    "ntd_params['min_goldstein_iters'] = 1\n",
    "ntd_params['use_trust_region'] = True\n",
    "print_every = 1\n",
    "\n",
    "# loss_f_softmax = unconstrained_loss(P_val) # softmax loss\n",
    "# history_ntd_unconstrained_random = ntd_method(h_params_ntd_unconstrained, loss_f_softmax max_iter, ntd_params, print_every, history_ntd_unconstrained_random)\n",
    "loss_f_normalized = scale_invariant_h_squared_loss(P_val=600) # alternative loss \n",
    "history_ntd_unconstrained_random = ntd_method(h_params_ntd_unconstrained, loss_f_normalized, max_iter, ntd_params, print_every, history_ntd_unconstrained_random)\n",
    "# loss_f_normalized_allow_negative = scale_invariant_h_squared_loss(P_val=600)\n",
    "# history_ntd_unconstrained_random = ntd_method(h_params_ntd_unconstrained, loss_f_normalized_allow_negative, max_iter, ntd_params, print_every, history_ntd_unconstrained_random)\n",
    "\n",
    "print(\"Best loss found:\", history_ntd_unconstrained_random['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_ntd_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"NTD method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_ntd_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"NTD method: Gradient Norm History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prox linear method \n",
    "\n",
    "Basic idea: The objective function is a max of smooth function.\n",
    "\n",
    "$$\n",
    "L(h) = \\max_{i} \\ell_i(h)\n",
    "$$\n",
    "\n",
    "So we linearize them smooth function and then minimize the linearization over the constraint. In particular, given current iterate $h$, we solve:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_+ = &\\mathrm{argmin}_{h'} \\max_{i} \\{ \\ell_i(h) + \\nabla \\ell_i(h) (h' - h) \\} + \\frac{\\gamma}{2} \\|h' - h\\|^2\\\\\n",
    "&\\text{subject to }  h' \\in \\Delta_{2P}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: \n",
    "- For large enough $\\gamma$ (bigger than maximal lipschitz constant of gradient of $\\ell_i$), each step is guaranteed to reduce the loss (if not at critical point), but progress is slower.\n",
    "- For $\\gamma = 0$, it takes bigs steps, good for initial iterates, but can start to cycle. \n",
    "- Lowest sol'n so far found is 1.5160005027, and i'm still running the method. It uses 1000 heights instead of 600.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "P_val = 600\n",
    "h_params_prox_linear = torch.rand(P_val, dtype=torch.float64)\n",
    "# h_params_prox_linear = initialize_from_matolcsi_vinuesa_best_step_208(2*P_val) #Need n = 208, but get's lowest loss.\n",
    "\n",
    "# h_params_prox_linear = h_params_google.detach().clone() # Start from google's solution if you want.\n",
    "h_params_prox_linear.data = 2*P_val*h_params_prox_linear.data/sum(h_params_prox_linear.data)\n",
    "h_params_prox_linear.requires_grad = True\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "file_path = 'best_found_so_far_prox_linear_P_val_' + str(P_val) + '.pt'\n",
    "if os.path.exists(file_path):\n",
    "    saved_best_h_params = torch.load(file_path)\n",
    "    if 'history_prox_linear' not in locals():\n",
    "        history_prox_linear = {}\n",
    "        history_prox_linear['best_h_params'] = saved_best_h_params\n",
    "        h_params_prox_linear.data = saved_best_h_params.data.clone()\n",
    "    else:\n",
    "        if loss_fn(saved_best_h_params) > (loss_fn(history_prox_linear['best_h_params'])):\n",
    "            torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "            h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "            print(\"Current point better than point in file. Saving current point.\")\n",
    "        else:\n",
    "            h_params_prox_linear.data = saved_best_h_params.data.clone()\n",
    "            print(\"Point in file better than current point. Loading point in file.\")\n",
    "else:\n",
    "    if 'history_prox_linear' not in locals():\n",
    "        history_prox_linear = {}\n",
    "    else: \n",
    "        torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "        h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "        print(\"No point in file. Saving current point.\")\n",
    "\n",
    "max_iter = 1 # try to run around 10-30 iterations at a time. can always pause and resume.\n",
    "delta_x = 0.5 / P_val\n",
    "# gamma_prox = delta_x makes sense since this is the weak convex parameter of the function.\n",
    "gamma_prox = delta_x # initially set to zero, but then try .00001, .0001, .001, etc. set as low as possible while you still make progress.\n",
    "print_every = 1\n",
    "# Solver parameters. Seems like CLARABEL is better for this problem.\n",
    "## For ECOS, Set error tolerances around 1e-9, then gradually decrease.\n",
    "# prox_linear_params = {'solver' : 'ECOS', 'gamma_prox': gamma_prox, 'abstol': 1e-10, 'reltol': 1e-09, 'feastol': 1e-09, 'max_iters': 2000, 'verbose': False}\n",
    "## Clarabel can handle much lower tolerances.\n",
    "tol = 1e-17\n",
    "prox_linear_params = {'solver' : 'CLARABEL', 'gamma_prox': gamma_prox, 'abstol': tol, 'reltol': tol, 'feastol': tol, 'max_iters': 2000, 'verbose': False}\n",
    "history_prox_linear = prox_linear_method(h_params_prox_linear, loss_fn, print_every, max_iter, prox_linear_params, history_prox_linear)\n",
    "\n",
    "print(\"Best loss found:\", history_prox_linear['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_prox_linear['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Proximal linear method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_prox_linear['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"Proximal linear method: Gradient Norm History\")\n",
    "plt.show()\n",
    "\n",
    "torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "print(\"No point in file. Saving current point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot autococonvolution of the best found so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_best_h_params = history_prox_linear['best_h_params']\n",
    "P_val = saved_best_h_params.shape[0]\n",
    "file_path = 'best_found_so_far_prox_linear_P_val_' + str(P_val) + '.pt'\n",
    "saved_best_h_params = torch.load(file_path)\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val \n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "autoconv_knot_vals_prox_linear = compute_autoconvolution_values(saved_best_h_params, f_delta_x, P_val)\n",
    "max_autoconv_value_prox_linear = torch.max(autoconv_knot_vals_prox_linear)\n",
    "\n",
    "plot_rendered_step_function(saved_best_h_params.detach().numpy(), f_interval, title=f\"Prox-linear Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_prox_linear.detach().numpy(), title=f\"Prox-linear Autoconvolution f*f(t) (P={P_val})\")\n",
    "print(sum(saved_best_h_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP approach described in [Matolcsi and Vinuesa (2010)](https://www.sciencedirect.com/science/article/pii/S0022247X10006001)\n",
    "\n",
    "Current best from random initialization: 1.5122793377 using 600 pieces. \n",
    "\n",
    "Note: the original paper used some closer to 208 pieces. You can start the method from this solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "P_val = 208\n",
    "# h_params_lp = torch.rand(P_val, dtype=torch.float64)\n",
    "# h_params_lp = initialize_from_matolcsi_vinuesa_negative_step_40(2*P_val)\n",
    "# h_params_lp = initialize_from_matolcsi_vinuesa_analytic(P_val) # Can't improve much on this for n = 208\n",
    "h_params_lp = initialize_from_matolcsi_vinuesa_best_step_208(2*P_val) #Need n = 208, but get's lowest loss.\n",
    "# make first half ones and second half random\n",
    "# h_params_lp= h_params_google.detach().clone() # Start from google's solution if you want.\n",
    "h_params_lp.data = 2*P_val*h_params_lp.data/sum(h_params_lp.data)\n",
    "h_params_lp.requires_grad = True\n",
    "loss_fn = constrained_loss(P_val=P_val)\n",
    "\n",
    "file_path = 'best_found_so_far_lp_P_val_' + str(P_val) + '.pt'\n",
    "if os.path.exists(file_path):\n",
    "    saved_best_h_params_lp = torch.load(file_path)\n",
    "    if 'history_lp' not in locals():\n",
    "        history_lp = {}\n",
    "        history_lp['best_h_params'] = saved_best_h_params_lp\n",
    "        h_params_lp.data = saved_best_h_params_lp.data.clone()\n",
    "    else:\n",
    "        print(history_lp['best_h_params'].shape[0])\n",
    "        if loss_fn(saved_best_h_params_lp) > (loss_fn(history_lp['best_h_params'])):\n",
    "            torch.save(history_lp['best_h_params'], file_path)\n",
    "            h_params_lp.data = history_lp['best_h_params'].data.clone()\n",
    "            print(\"Current point better than point in file. Saving current point.\")\n",
    "        else:\n",
    "            h_params_lp.data = saved_best_h_params_lp.data.clone()\n",
    "            print(\"Point in file better than current point. Loading point in file.\")\n",
    "else:\n",
    "    if 'history_lp' not in locals():\n",
    "        history_lp = {}\n",
    "    else: \n",
    "        torch.save(history_lp['best_h_params'], file_path)\n",
    "        print(\"size of h_params_lp: \", h_params_lp.shape[0])\n",
    "        print(\"No point in file. Saving current point.\")\n",
    "\n",
    "max_iter = 100 # try to run around 10-30 iterations at a time. can always pause and resume.\n",
    "print_every = 1\n",
    "line_search_iters = 40\n",
    "# Solver parameters. Seems like CLARABEL is better for this problem.\n",
    "## For ECOS, Set error tolerances around 1e-9, then gradually decrease.\n",
    "# lp_params = {'solver' : 'ECOS', 'abstol': 1e-10, 'reltol': 1e-09, 'feastol': 1e-09, 'max_iters': 2000, 'verbose': False}\n",
    "## Clarabel can handle much lower tolerances.\n",
    "tol = 1e-17\n",
    "lp_params = {'solver' : 'CLARABEL', 'abstol': tol, 'reltol': tol, 'feastol': tol, 'max_iters': 2000, 'verbose': False, 'line_search_iters' : line_search_iters}\n",
    "history_lp = lp_method(h_params_lp, loss_fn, print_every, max_iter, lp_params, history_lp)\n",
    "\n",
    "print(\"Best loss found:\", history_lp['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_lp['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LP method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_lp['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"LP method: Gradient Norm History\")\n",
    "plt.show()\n",
    "\n",
    "# file_path = 'best_found_so_far_prox_linear_P_val_' + str(P_val) + '.pt'\n",
    "torch.save(history_lp['best_h_params'], file_path)\n",
    "h_params_lp.data = history_lp['best_h_params'].data.clone()\n",
    "print(\"No point in file. Saving current point.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_best_h_params = history_lp['best_h_params']\n",
    "saved_best_h_params = saved_best_h_params.detach().clone()/sum(saved_best_h_params)\n",
    "P_val = saved_best_h_params.shape[0]\n",
    "file_path = 'best_found_so_far_lp_P_val_' + str(P_val) + '.pt'\n",
    "saved_best_h_params = torch.load(file_path)\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val \n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "autoconv_knot_vals_prox_linear = compute_autoconvolution_values(saved_best_h_params, f_delta_x, P_val)\n",
    "max_autoconv_value_prox_linear = torch.max(autoconv_knot_vals_prox_linear)\n",
    "\n",
    "plot_rendered_step_function(saved_best_h_params.detach().numpy(), f_interval, title=f\"LP Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_prox_linear.detach().numpy(), title=f\"LP Autoconvolution f*f(t) (P={P_val})\")\n",
    "print(sum(saved_best_h_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semidefinite Programming (SDP) Relaxation for Autoconvolution Peak Minimization\n",
    "\n",
    "The problem is to find a non-negative step function $f_h(x)$ (defined by $P$ heights $h_i \\ge 0$) supported on an interval (e.g., $[-1/4, 1/4]$) such that its integral $\\int f_h(x)dx = 1$, which minimizes the maximum value of its autoconvolution: $\\min_{h} \\max_{m} (f_h * f_h)(t_m)$.\n",
    "\n",
    "Let $h = (h_0, \\ldots, h_{P-1})$ be the vector of heights.\n",
    "The constraint $\\int f_h(x)dx = 1$ translates to $\\sum_{i=0}^{P-1} h_i = S_{target}$ (where $S_{target} = 1/\\Delta x$, and $\\Delta x$ is the width of each step).\n",
    "The autoconvolution values at the discrete knots $t_m$ (for $m=1, \\ldots, 2P-1$) can be written as quadratic forms $Q_m(h) = h^T K_m h$, where $K_m$ are specific symmetric $P \\times P$ matrices derived from the discrete convolution structure.\n",
    "\n",
    "The original problem is:\n",
    "$$ \\min_{\\eta, h} \\quad \\eta $$\n",
    "Subject to:\n",
    "1.  $h^T K_m h \\le \\eta \\quad \\forall m \\in \\{1, \\ldots, 2P-1\\}$\n",
    "2.  $\\sum_{i=0}^{P-1} h_i = S_{target}$\n",
    "3.  $h_i \\ge 0 \\quad \\forall i$\n",
    "\n",
    "This is a non-convex quadratically constrained problem. To obtain a lower bound, we formulate an SDP relaxation.\n",
    "\n",
    "**SDP Relaxation:**\n",
    "We introduce a symmetric matrix variable $X \\in \\mathbb{S}^{P \\times P}$ and use the lifting $h^T K_m h = \\text{Trace}(K_m X)$. The non-convex constraint $X = hh^T$ is relaxed to $X \\succeq hh^T$, which is equivalent to the Linear Matrix Inequality (LMI):\n",
    "$$ \\begin{pmatrix} X & h \\\\ h^T & 1 \\end{pmatrix} \\succeq 0 $$\n",
    "This LMI also implies $X \\succeq 0$.\n",
    "\n",
    "The SDP relaxation is:\n",
    "$$ \\min_{\\eta, X, h} \\quad \\eta $$\n",
    "Subject to:\n",
    "1.  $\\text{Trace}(K_m X) \\le \\eta \\quad \\forall m \\in \\{1, \\ldots, 2P-1\\}$ (Linear constraints)\n",
    "2.  $\\begin{pmatrix} X & h \\\\ h^T & 1 \\end{pmatrix} \\succeq 0$ (Semidefinite constraint)\n",
    "3.  $\\sum_{i=0}^{P-1} h_i = S_{target}$ (Linear constraint)\n",
    "4.  $h_i \\ge 0 \\quad \\forall i$ (Linear constraints)\n",
    "\n",
    "**Additional Tightening Constraints (Optional but often helpful):**\n",
    "To improve the quality (tightness) of the lower bound obtained from the SDP, further valid linear constraints derived from the properties of $X=hh^T$ and $h_i \\ge 0, \\sum h_i = S_{target}$ can be added:\n",
    "5.  $X_{ij} \\ge 0 \\quad \\forall i,j$ (element-wise non-negativity of $X$)\n",
    "6.  $\\sum_{i,j} X_{ij} = S_{target}^2$ (equivalent to $\\text{Trace}(\\mathbf{J}X) = S_{target}^2$, where $\\mathbf{J}$ is the matrix of all ones)\n",
    "7.  $\\sum_{j=0}^{P-1} X_{ij} = S_{target} h_i \\quad \\forall i$ (RLT constraint from $\\sum h_j = S_{target}$ multiplied by $h_i$)\n",
    "8.  $X_{ij} \\le S_{target} h_i$ and $X_{ij} \\le S_{target} h_j \\quad \\forall i,j$ (RLT constraints from $h_k \\le S_{target}$ multiplied by $h_i$ or $h_j$)\n",
    "9.  $\\text{Trace}((\\Delta x \\sum_{m=1}^{2P-1} K_m) X) = 1$ (Enforces $\\int (f_h*f_h)(t)dt \\approx 1$, where the 1 comes from $(\\int f_h dx)^2 = 1^2$)\n",
    "\n",
    "The optimal value $\\eta^*$ of this SDP provides a lower bound to the true minimum of the original problem. If the optimal $X^*$ is rank-1, the bound is tight. Otherwise, it's a strict lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# --- Example Usage (conceptual, might be slow for large P_val) ---\n",
    "P_val_test_sdp = 50 # Keep P small for SDP testing unless you have a powerful machine and solver\n",
    "S_target_test_sdp = 2.0 * P_val_test_sdp\n",
    "delta_x_test_sdp = 0.5 / P_val_test_sdp\n",
    "\n",
    "sdp_solver_settings = {\n",
    "    'solver': 'CLARABEL', \n",
    "    'verbose': True,\n",
    "    'tol_gap_abs': 1e-7, \n",
    "    'tol_gap_rel': 1e-7, \n",
    "    'tol_feas': 1e-7,\n",
    "    'max_iter': 500 # Clarabel uses max_iter\n",
    "}\n",
    "# For MOSEK (if installed and licensed):\n",
    "# sdp_solver_settings = {'solver': 'MOSEK', 'verbose': True} \n",
    "\n",
    "print(f\"Attempting SDP relaxation for P={P_val_test_sdp}...\")\n",
    "lower_bound_eta, h_sol_sdp, X_sol_sdp = solve_sdp_relaxation_lower_bound(\n",
    "    P_val_test_sdp, S_target_test_sdp, delta_x_test_sdp, solver_params=sdp_solver_settings\n",
    ")\n",
    "\n",
    "if lower_bound_eta is not None:\n",
    "    print(f\"\\nSDP Lower Bound (eta*): {lower_bound_eta:.7f}\")\n",
    "    if h_sol_sdp is not None:\n",
    "        print(f\"  h vector from SDP (first 5): {h_sol_sdp.numpy()[:5]}\")\n",
    "        print(f\"  Sum of h from SDP: {torch.sum(h_sol_sdp).item()}\")\n",
    "    if X_sol_sdp is not None:\n",
    "        print(f\"  X matrix from SDP (top-left 3x3):\\n{X_sol_sdp.numpy()[:3,:3]}\")\n",
    "        \n",
    "        # Check rank of X (if relaxation is tight, rank should be 1)\n",
    "        # This requires X_sol_sdp to be converted to numpy if it's a torch tensor\n",
    "        # X_np_for_rank = X_sol_sdp.numpy() if isinstance(X_sol_sdp, torch.Tensor) else X_sol_sdp\n",
    "        try:\n",
    "            # Compute rank based on singular values\n",
    "            s_vals = np.linalg.svd(X_sol_sdp.numpy(), compute_uv=False)\n",
    "            rank_X = np.sum(s_vals > 1e-6) # Count singular values significantly larger than zero\n",
    "            print(f\"  Approximate Rank of X from SDP: {rank_X}\")\n",
    "            if rank_X == 1:\n",
    "                print(\"  SDP relaxation appears to be tight (Rank(X) approx 1)!\")\n",
    "            else:\n",
    "                print(\"  SDP relaxation is likely not tight (Rank(X) > 1).\")\n",
    "        except Exception as e_rank:\n",
    "            print(f\"  Could not compute rank of X: {e_rank}\")\n",
    "else:\n",
    "    print(\"SDP relaxation failed to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(h_sol_sdp)\n",
    "print(\"loss\", constrained_loss(h_sol_sdp.shape[0])(h_sol_sdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = h_sol_sdp.shape[0]\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val \n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "autoconv_knot_vals_prox_linear = compute_autoconvolution_values(h_sol_sdp, f_delta_x, P_val)\n",
    "max_autoconv_value_prox_linear = torch.max(autoconv_knot_vals_prox_linear)\n",
    "\n",
    "plot_rendered_step_function(h_sol_sdp.detach().numpy(), f_interval, title=f\"SDP Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_prox_linear.detach().numpy(), title=f\"SDP Autoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 40\n",
    "h_params_lp = initialize_from_matolcsi_vinuesa_negative_step_40(2*P_val)\n",
    "sum(h_params_lp)\n",
    "print(torch.clamp(h_params_lp, min=0.0))\n",
    "clamped = torch.clamp(h_params_lp, min=0.001)\n",
    "print(\"loss\", constrained_loss(P_val)(2*P_val * clamped/sum(clamped)))\n",
    "P_val = h_params_lp.shape[0]\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val \n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "autoconv_knot_vals_lp = compute_autoconvolution_values(clamped, f_delta_x, P_val)\n",
    "max_autoconv_value_lp = torch.max(autoconv_knot_vals_lp)\n",
    "\n",
    "plot_rendered_step_function(clamped.detach().numpy(), f_interval, title=f\"LP Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_lp.detach().numpy(), title=f\"LP Autoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the derivative of the autoconvolution\n",
    "\n",
    "By the fundamental theorem of calculus you can bound the max of a function as follows: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{-1/2 \\leq t \\leq 1/2} g(t) &= g(-1/2) + \\max_{-1/2 \\leq t \\leq 1/2} \\int_{-1/2}^t g'(t) \\\\\n",
    "&\\leq g(-1/2) +  \\int_{-1/2}^{1/2} |g'(t)|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the following code tries to minimize the integral of the absolute value of the derivative of the function (we ignore the constant $g(-1/2)$. It doesn't work that well, but it does produce a similar structure to the step function solutions. Namely the derivative is encouraged to be near zero, so th function has a large plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "P_val = 2000\n",
    "h_params_bfgs_unconstrained = (torch.randn(P_val).abs() * 0.01 + 0.1).log()\n",
    "# h_params_bfgs_unconstrained = torch.randn(P_val)\n",
    "# h_params_bfgs_unconstrained = h_params_google # Start from google's solution if you want.\n",
    "# h_params_bfgs_unconstrained.data = torch.log(h_params_bfgs_unconstrained.data)\n",
    "h_params_bfgs_unconstrained.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "if 'history_bfgs_unconstrained_random' not in locals():\n",
    "    history_bfgs_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_bfgs_unconstrained.data = history_bfgs_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 2000\n",
    "bfgs_params = {}\n",
    "bfgs_params['lr'] = 1\n",
    "bfgs_params['history_size'] = 2000\n",
    "bfgs_params['ls_params'] = {'c1': 0.0001, 'c2': 0.5}\n",
    "# bfgs_params['line_search_fn'] = None # no line search function. Will likely blow up.\n",
    "bfgs_params['line_search_fn'] = 'weak_wolfe' # good for later iterations\n",
    "print_every = 10\n",
    "\n",
    "loss_f_softmax = unconstrained_loss_TV_of_autoconv(P_val)\n",
    "history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_softmax, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "# loss_f_normalized = scale_invariant_h_squared_loss(P_val=600)\n",
    "# history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_normalized, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "# loss_f_normalized_allow_negative = scale_invariant_h_squared_loss(P_val=600)\n",
    "# history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_normalized_allow_negative, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "\n",
    "\n",
    "print(\"Best loss found:\", history_bfgs_unconstrained_random['min_loss_found'])\n",
    "\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_bfgs_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"BFGS method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_bfgs_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"BFGS method: Gradient Norm History\")\n",
    "plt.show()\n",
    "\n",
    "saved_best_h_params = 2*P_val*F.softmax(history_bfgs_unconstrained_random['best_h_params'], dim=0)\n",
    "P_val = saved_best_h_params.shape[0]\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val \n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "autoconv_knot_vals_prox_linear = compute_autoconvolution_values(saved_best_h_params, f_delta_x, P_val)\n",
    "max_autoconv_value_prox_linear = torch.max(autoconv_knot_vals_prox_linear)\n",
    "\n",
    "plot_rendered_step_function(saved_best_h_params.detach().numpy(), f_interval, title=f\"bfgs unconstrained Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_prox_linear.detach().numpy(), title=f\"bfgs unconstrained Autoconvolution f*f(t) (P={P_val})\")\n",
    "print(sum(saved_best_h_params))\n",
    "\n",
    "loss_fn = constrained_loss(P_val)\n",
    "print(\"Loss\", loss_fn(saved_best_h_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Gaussians\n",
    "\n",
    "Here is an alternative strategy: start with a pdf $f$ which is a Gaussian mixture, then note that the autoconvolution is another gaussian mixture. We can evaluate the maximum of the autoconvolution over a grid to approximate the value at the mode. Weoptimize over the mean, variance, and weights of the gaussian mixture. Then after we fit the model, we discretize back to a step function and plot. It doesn't work that well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "P_val = 600\n",
    "\n",
    "K_components = 50\n",
    "N_grid_eval = 2*P_val # Number of points to evaluate autoconv for max\n",
    "sigma_min_init = 0.0000001\n",
    "sigma_max_init = 0.08 # (0.5 / 6.01) approx\n",
    "lambda_sigma_upper_bound = 10\n",
    "lambda_mu_bounds = 10\n",
    "\n",
    "loss_fn_v3 = gaussian_mixture_max_autoconv_loss_v3_learnable_sigma(\n",
    "    num_mixture_components=K_components,\n",
    "    num_grid_points_autocorr=N_grid_eval,\n",
    "    lambda_sigma_upper_bound=lambda_sigma_upper_bound,\n",
    "    lambda_mu_bounds=lambda_mu_bounds,\n",
    "    min_sigma_init_val=sigma_min_init,\n",
    "    max_sigma_init_val=sigma_max_init\n",
    ")\n",
    "\n",
    "initial_log_weights = torch.randn(K_components, dtype=torch.float64)\n",
    "initial_raw_means = torch.rand(K_components, dtype=torch.float64) * 0.4 - 0.2 # Start somewhat centered\n",
    "# Initialize raw_log_sigmas so sigmas start in a reasonable range, e.g., around 0.03\n",
    "initial_raw_log_sigmas = torch.log(torch.full((K_components,), 0.03, dtype=torch.float64))\n",
    "\n",
    "combined_params = torch.cat([\n",
    "    initial_log_weights, \n",
    "    initial_raw_means, \n",
    "    initial_raw_log_sigmas\n",
    "]).requires_grad_(True)\n",
    "\n",
    "# Alternative: load the 50 component solution from the previous cell.\n",
    "combined_params = torch.load('gmm_50components_2000P_1000iter.pt')\n",
    "combined_params = combined_params['best_h_params']\n",
    "print(combined_params)\n",
    "loss_val = loss_fn_v3(combined_params)\n",
    "print(f\"V3 Loss (learnable sigma): {loss_val.item()}\")\n",
    "\n",
    "\n",
    "h_params_bfgs_unconstrained = combined_params\n",
    "h_params_bfgs_unconstrained.requires_grad = True\n",
    "\n",
    "if 'history_bfgs_unconstrained_random' not in locals():\n",
    "    history_bfgs_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_bfgs_unconstrained.data = history_bfgs_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "if K_components != 2*h_params_bfgs_unconstrained.shape[0]:\n",
    "    history_bfgs_unconstrained_random = {}\n",
    "\n",
    "\n",
    "max_iter = 10\n",
    "bfgs_params = {}\n",
    "bfgs_params['lr'] = 10\n",
    "bfgs_params['history_size'] = 1000\n",
    "bfgs_params['ls_params'] = {'c1': 0.0001, 'c2': 0.9}\n",
    "# bfgs_params['line_search_fn'] = None # no line search function. Will likely blow up.\n",
    "bfgs_params['line_search_fn'] = 'weak_wolfe' # good for later iterations\n",
    "print_every = 10\n",
    "\n",
    "history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_fn_v3, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "\n",
    "print(h_params_bfgs_unconstrained)\n",
    "\n",
    "print(\"Best loss found:\", history_bfgs_unconstrained_random['min_loss_found'])\n",
    "\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_bfgs_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"BFGS method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_bfgs_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"BFGS method: Gradient Norm History\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_params = history_bfgs_unconstrained_random['best_h_params']\n",
    "\n",
    "step_h = convert_gaussian_mixture_v3_to_step_function(\n",
    "    combined_params=history_bfgs_unconstrained_random['best_h_params'],\n",
    "    num_mixture_components=K_components,\n",
    "    P_val_step_func=P_val,\n",
    "    S_target_step_func=2*P_val,\n",
    "    f_interval_min=-0.25,\n",
    "    f_interval_max=0.25,\n",
    "    min_sigma_init_val=sigma_min_init,\n",
    "    max_sigma_init_val=sigma_max_init\n",
    ")\n",
    "\n",
    "f_x_min = -0.25\n",
    "f_x_max = 0.25\n",
    "delta_x_final_step = (f_x_max - f_x_min) / P_val\n",
    "integral_of_final_step = torch.sum(step_h * delta_x_final_step).item()\n",
    "print(f\"Integral of discretized step function: {integral_of_final_step:.4f} (Target for PDF: 1.0)\")\n",
    "\n",
    "# You can then plot step_h or compute its autoconvolution using your existing utils\n",
    "plot_rendered_step_function(step_h.detach().numpy(), (-0.25,0.25), \"Step Func from Gaussian Mixture\")\n",
    "autoconv_step = compute_autoconvolution_values(step_h.detach(), delta_x_final_step, P_val)\n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "\n",
    "max_autoconv_step = torch.max(autoconv_step[1:-1]).item()\n",
    "print(f\"Max autoconv of this step function: {max_autoconv_step}\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_step.detach().numpy(), title=f\"Pdf Autoconvolution f*f(t) (P={P_val})\")\n",
    "\n",
    "loss_fn = constrained_loss(P_val)\n",
    "print(\"loss of the step function:\",loss_fn(step_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_params = torch.load('gmm_50components_2000P_1000iter.pt')\n",
    "combined_params['best_h_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
