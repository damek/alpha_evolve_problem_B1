{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B1: First autocorrelation inequality\n",
    "For any function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$, define the *autoconvolution* of $f$, written $f*f$, as\n",
    "$$f*f (t) := \\int_\\mathbb{R} f(t-x) f(x)\\ dx.$$\n",
    "\n",
    "Let $C_1$ denote the largest constant for which one has\n",
    "\\begin{equation}\n",
    " \\max_{-1/2 \\leq t \\leq 1/2} f*f(t) \\geq C_1 \\left(\\int_{-1/4}^{1/4} f(x)\\ dx\\right)^2\n",
    "\\end{equation}\n",
    "for all non-negative $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.  This problem arises in additive combinatorics, relating to the size of Sidon sets.  It is currently known that\n",
    "$$ 1.28 \\leq C_1 \\leq 1.5098$$\n",
    "with the lower bound proven by [Cloninger and Steinerberger (2017)](https://www.ams.org/journals/proc/2017-145-08/S0002-9939-2017-13690-9/S0002-9939-2017-13690-9.pdf) and the upper bound achieved by [Matolcsi and Vinuesa (2010)](https://www.sciencedirect.com/science/article/pii/S0022247X10006001) via a step function construction. AlphaEvolve found a step function with 600 equally-spaced intervals on $[-1/4,1/4]$ that gives a better upper bound of $C_1 \\leq 1.5053$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the step function, the integral, and the autoconvolution\n",
    "\n",
    "For step func. \n",
    "$$f(x) = \\sum_{i=0}^{P-1} h_i \\mathbf{1}_{[x_i, x_{i+1})}(x)$$  \n",
    "where\n",
    "$$ x_i = -\\frac{1}{4} + \\frac{i}{2P}, \\text{ and heights } h_i \\ge 0:$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "% Integral\n",
    "\\text{Integral:}\\quad \\int_{-1/4}^{1/4} f(x) \\,dx &= \\frac{1}{2P} \\sum_{i=0}^{P-1} h_i\n",
    "\\\\[2ex]\n",
    "% Autoconvolution\n",
    "\\text{Autoconv. knots:}\\quad (f*f)(t_m) &= \n",
    "\\begin{cases} \n",
    "0 & \\text{if } m=0 \\text{ or } m=2P \\\\\n",
    "\\frac{1}{2P} \\sum_{k=\\max(0, m-P)}^{\\min(P-1, m-1)} h_k h_{m-1-k} & \\text{if } 1 \\le m \\le 2P-1\n",
    "\\end{cases}\n",
    "\\\\[0.5ex]\n",
    "&\\quad \\text{where } t_m = -\\frac{1}{2} + \\frac{m}{2P} \\quad (m=0, \\dots, 2P).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the step function and the autoconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script assumes 'compute_integral_of_step_function' and 'compute_autoconvolution_values'\n",
    "# (from your previous code block) are already defined and executable in the notebook.\n",
    "from utils import *\n",
    "\n",
    "# --- main script logic ---\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "\n",
    "# 1. sample random heights (non-negative)\n",
    "# using float64 for precision, requires_grad=False for plotting/evaluation only\n",
    "height_values = torch.rand(P_val, dtype=torch.float64) \n",
    "# for more structured/sparse functions, you could try:\n",
    "# height_values = torch.abs(torch.randn(P_val, dtype=torch.float64)) * (torch.rand(P_val, dtype=torch.float64) > 0.7).double()\n",
    "\n",
    "\n",
    "# 2. compute integral of f(x)\n",
    "integral_value = compute_integral_of_step_function(height_values, f_delta_x)\n",
    "\n",
    "# 3. compute autoconvolution (f*f)(t) knot values\n",
    "# (2*P_val + 1) values for t_m from 2*f_x_min to 2*f_x_max\n",
    "autoconv_knot_vals = compute_autoconvolution_values(height_values, f_delta_x, P_val)\n",
    "max_autoconv_value = torch.max(autoconv_knot_vals)\n",
    "\n",
    "# 4. calculate the ratio for C1 estimate\n",
    "# (max f*f(t)) / (integral f(x) dx)^2\n",
    "if integral_value.item() == 0:\n",
    "    c1_ratio = float('inf') if max_autoconv_value.item() > 0 else 0.0 # handle division by zero\n",
    "else:\n",
    "    c1_ratio = max_autoconv_value / (integral_value**2)\n",
    "\n",
    "print(f\"--- Function Details (P={P_val}) ---\")\n",
    "# print(f\"Heights (h_i, first 10): {height_values.numpy()[:10]}\") # uncomment if you want to see heights\n",
    "print(f\"Integral of f(x): {integral_value.item():.6f}\")\n",
    "print(f\"Max value of autoconvolution (f*f)(t): {max_autoconv_value.item():.6f}\")\n",
    "print(f\"Ratio max(f*f) / (integral(f))^2: {c1_ratio.item():.6f}\")\n",
    "\n",
    "\n",
    "# 5. plotting\n",
    "# plot f(x)\n",
    "plot_rendered_step_function(height_values.numpy(), f_interval, title=f\"Random Step Function f(x) (P={P_val})\")\n",
    "\n",
    "# plot f*f(t)\n",
    "# autoconvolution is defined on [2*f_x_min, 2*f_x_max]\n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals.numpy(), title=f\"Autoconvolution f*f(t) (P={P_val})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Google's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "h_params_google = return_height_params_google_simplex()\n",
    "autoconv_knot_vals_google = compute_autoconvolution_values(h_params_google, f_delta_x, P_val)\n",
    "max_autoconv_value_google = torch.max(autoconv_knot_vals_google)\n",
    "\n",
    "plot_rendered_step_function(h_params_google.detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google.detach().numpy(), title=f\"Google'sAutoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: (Projected) Polyak subgradient method\n",
    "\n",
    "The algorithm \n",
    "$$\n",
    "h_+ = \\mathrm{proj}_{\\Delta_{2P}}\\left(h - \\frac{L(h) - L^*}{\\| \\nabla L(h) \\|^2} \\nabla L(h)\\right)\n",
    "$$\n",
    "where $L^*$ is the target loss.\n",
    "\n",
    "There is a subtlety here: we don't know what the optimal value is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak with Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "# Initialize random height parameters\n",
    "h_params_polyak = torch.rand(P_val, dtype=torch.float64)\n",
    "h_params_polyak.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak.data = projection(h_params_polyak.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "\n",
    "# Setting up history\n",
    "if 'loss_history_polyak' not in locals():\n",
    "    loss_history_polyak = []\n",
    "if 'grad_norm_history_polyak' not in locals():\n",
    "    grad_norm_history_polyak = []\n",
    "if 'min_loss_found_polyak' not in locals():\n",
    "    min_loss_found_polyak = float('inf')\n",
    "if 'best_h_params_polyak' not in locals():\n",
    "    best_h_params_polyak = h_params_polyak.data.clone()\n",
    "\n",
    "# Setting up parameters for Polyak\n",
    "max_iter = 100000\n",
    "target_loss = 1.505\n",
    "print_every = 1000\n",
    "\n",
    "best_height_params_polyak, min_loss_found_polyak, loss_history_polyak, grad_norm_history_polyak = polyak_subgradient_method(h_params_polyak,loss_fn, max_iter, target_loss, print_every, loss_history_polyak, grad_norm_history_polyak, min_loss_found_polyak, best_h_params_polyak, projection)\n",
    "print(\"Best loss found:\", min_loss_found_polyak)\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_polyak)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoconv_knot_vals_polyak = compute_autoconvolution_values(best_height_params_polyak, f_delta_x, P_val)\n",
    "max_autoconv_value_polyak = torch.max(autoconv_knot_vals_polyak)\n",
    "\n",
    "plot_rendered_step_function(best_height_params_polyak.detach().numpy(), f_interval, title=f\"Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_polyak.detach().numpy(), title=f\"Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak started at the Google solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "h_params_polyak_google = h_params_google.data.detach().clone() # initialize variable at google's solution.\n",
    "h_params_polyak_google.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak_google.data = projection(h_params_polyak_google.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "# Setting up history\n",
    "if 'loss_history_polyak_google' not in locals():\n",
    "    loss_history_polyak_google = []\n",
    "if 'grad_norm_history_polyak_google' not in locals():\n",
    "    grad_norm_history_polyak_google = []\n",
    "if 'min_loss_found_polyak_google' not in locals():\n",
    "    min_loss_found_polyak_google = float('inf')\n",
    "if 'best_h_params_polyak_google' not in locals():\n",
    "    best_h_params_polyak_google = h_params_polyak_google.data.clone()\n",
    "# if 'loss_history_polyak_google' not in locals():\n",
    "# Setting up parameters for Polyak\n",
    "max_iter = 10000\n",
    "target_loss_google = 1.505292\n",
    "print_every = 1000\n",
    "\n",
    "best_height_params_polyak_google, min_loss_found_polyak_google, loss_history_polyak_google, grad_norm_history_polyak_google = polyak_subgradient_method(h_params_polyak_google, loss_fn, max_iter, target_loss_google, print_every, loss_history_polyak_google, grad_norm_history_polyak_google, min_loss_found_polyak_google, best_h_params_polyak_google, projection)\n",
    "\n",
    "print(\"Best loss found (Google solution initialization):\", min_loss_found_polyak_google)\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_polyak_google)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method (Google solution initialization): Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "autoconv_knot_vals_google_polyak = compute_autoconvolution_values(best_height_params_polyak_google, f_delta_x, P_val)\n",
    "max_autoconv_value_google_polyak = torch.max(autoconv_knot_vals_google_polyak)\n",
    "\n",
    "plot_rendered_step_function(best_height_params_polyak_google.detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google_polyak.detach().numpy(), title=f\"Google's Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "\n",
    "print(\"best_height_params_polyak_google\", best_height_params_polyak_google)\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the loss function to remove constraints\n",
    "\n",
    "We're going to precompose the loss function with a softmax. This implicitly constrains the functions to the simplex. I.e., we will optimize\n",
    "\n",
    "$$\n",
    "L(2P\\mathrm{softmax}(w))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Setting function parameters\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "# Define optimization variable\n",
    "height_params = torch.rand(P_val, dtype=torch.float64)\n",
    "height_params.requires_grad = True\n",
    "\n",
    "def loss_fn_softmax(log_h): \n",
    "    vals = 2*P_val*F.softmax(log_h, dim=0)\n",
    "    max_f_conv_f_values_at_knots = compute_autoconvolution_values(vals, f_delta_x, P_val)\n",
    "    return max_f_conv_f_values_at_knots.max()\n",
    "\n",
    "# Compute loss and gradient at a random point.\n",
    "loss = loss_fn_softmax(height_params)\n",
    "print(\"loss\", loss.item())\n",
    "grad = torch.autograd.grad(loss, height_params)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyak on unconstrained (softmax) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "height_params_polyak = height_params.data.detach().clone() # initialize variable at value from test cell \n",
    "height_params_polyak.requires_grad = True\n",
    "\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history_polyak' not in locals():\n",
    "    loss_history_polyak = []\n",
    "\n",
    "if 'grad_norm_history_polyak' not in locals():\n",
    "    grad_norm_history_polyak = []\n",
    "max_iter = 10000\n",
    "target_loss = 1.505\n",
    "print_every = 1000\n",
    "\n",
    "if 'min_loss_found_polyak' not in locals():\n",
    "    min_loss_found_polyak = float('inf')\n",
    "\n",
    "if 'best_height_params_polyak' not in locals():\n",
    "    best_height_params_polyak = height_params_polyak.data.clone()\n",
    "else: \n",
    "    height_params_polyak.data = best_height_params_polyak.data.clone()\n",
    "\n",
    "print(\"Starting Polyak subgradient method\")\n",
    "best_height_params_polyak, min_loss_found_polyak, loss_history_polyak, grad_norm_history_polyak = polyak_subgradient_method(height_params_polyak, loss_fn_softmax,max_iter=max_iter, target_loss=target_loss, print_every=print_every, loss_history=loss_history_polyak, grad_norm_history=grad_norm_history_polyak, min_loss_found   =min_loss_found_polyak, best_h_params=best_height_params_polyak)\n",
    "\n",
    "print(\"Best loss found:\", min_loss_found_polyak)\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_polyak)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(grad_norm_history_polyak)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs_method_softmax(h_params, loss_fn, max_iter=100000, lr=1.0, print_every=100, loss_history=[], grad_history=[], min_loss_found=float('inf'), best_h_params=None):\n",
    "    optimizer = torch.optim.LBFGS([h_params], lr=lr)\n",
    "    print(\"Starting BFGS method\")\n",
    "    for i in range(max_iter):\n",
    "        # Compute loss and gradient\n",
    "        def closure():\n",
    "            h_params.grad = None\n",
    "            loss = loss_fn(h_params)\n",
    "            grad = torch.autograd.grad(loss, h_params)[0]\n",
    "            h_params.grad = grad\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        grad_norm_squared = torch.norm(grad)\n",
    "        loss_history.append(loss.item())        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"Iteration {i}: loss = {loss.item():.7f}, grad_norm = {torch.sqrt(grad_norm_squared).item():.7f}, min_loss_found = {min_loss_found:.6f}\",)\n",
    "\n",
    "        if loss.item() < min_loss_found:\n",
    "            min_loss_found = loss.item()\n",
    "            best_h_params = h_params.data.clone()\n",
    "    return best_h_params, min_loss_found, loss_history, grad_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "height_params_bfgs = height_params.data.detach().clone() # initialize variable at value from test cell \n",
    "height_params_bfgs.requires_grad = True\n",
    "\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history_bfgs' not in locals():\n",
    "    loss_history_bfgs = []\n",
    "\n",
    "max_iter = 100000\n",
    "target_loss = 1.52\n",
    "print_every = 1\n",
    "lr = 1.0\n",
    "\n",
    "if 'min_loss_found_bfgs' not in locals():\n",
    "    min_loss_found_bfgs = float('inf')\n",
    "\n",
    "if 'best_height_params_bfgs' not in locals():\n",
    "    best_height_params_bfgs = height_params_bfgs.data.clone()\n",
    "\n",
    "best_height_params_bfgs, min_loss_found_bfgs, loss_history_bfgs = bfgs_method_softmax(height_params_bfgs, loss_fn_softmax, max_iter, lr=lr, print_every=print_every, loss_history=loss_history_bfgs, min_loss_found=min_loss_found_bfgs, best_h_params=best_height_params_bfgs)\n",
    "\n",
    "print(\"Best loss found:\", min_loss_found_bfgs)\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_bfgs)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"BFGS method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "height_params_polyak = height_params.data.detach().clone() # initialize variable at value from test cell \n",
    "height_params_polyak.requires_grad = True\n",
    "\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history_polyak' not in locals():\n",
    "    loss_history_polyak = []\n",
    "\n",
    "if 'grad_history_polyak' not in locals():\n",
    "    grad_history_polyak = []\n",
    "max_iter = 10000\n",
    "target_loss = 1.505\n",
    "print_every = 1000\n",
    "\n",
    "if 'min_loss_found_polyak' not in locals():\n",
    "    min_loss_found_polyak = float('inf')\n",
    "\n",
    "if 'best_height_params_polyak' not in locals():\n",
    "    best_height_params_polyak = height_params_polyak.data.clone()\n",
    "else: \n",
    "    height_params_polyak.data = best_height_params_polyak.data.clone()\n",
    "\n",
    "print(\"Starting Polyak subgradient method\")\n",
    "best_height_params_polyak, min_loss_found_polyak, loss_history_polyak, grad_history_polyak = polyak_subgradient_method_softmax(height_params_polyak, loss_fn_softmax, max_iter, target_loss, print_every, loss_history_polyak, grad_history_polyak, min_loss_found_polyak, best_height_params_polyak)\n",
    "\n",
    "print(\"Best loss found:\", min_loss_found_polyak)\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_polyak)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(grad_history_polyak)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(h_params, loss_fn, max_iter=100000, target_loss=1.5053, print_every=100, loss_history_newton=[], min_loss_found_newton=float('inf'), best_height_params_newton=height_params.data.clone()):\n",
    "    for i in range(max_iter):\n",
    "        # Compute loss and gradient\n",
    "        h_params.grad = None\n",
    "        loss = loss_fn_softmax(h_params)\n",
    "        grad = torch.autograd.grad(loss, h_params, create_graph=False)[0]\n",
    "        hessian = torch.autograd.functional.hessian(loss_fn_softmax, h_params, create_graph=False)\n",
    "\n",
    "        reg_hessian = torch.sqrt(10*torch.linalg.norm(grad))*torch.eye(torch.numel(h_params)) + hessian\n",
    "        try:\n",
    "            direction = torch.linalg.solve(reg_hessian, grad)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            # if hessian is singular, use pseudo-inverse (least-squares solution)\n",
    "            # this provides the minimum norm solution when the system is consistent and underdetermined\n",
    "            # or the minimum norm least-squares solution if inconsistent\n",
    "            direction = torch.linalg.lstsq(reg_hessian, grad).solution\n",
    "        h_params.data -= direction\n",
    "\n",
    "        # Project onto the simplex\n",
    "        # with torch.no_grad():\n",
    "            # h_params.data = projection_simplex_pytorch(h_params.data, 2*P_val)\n",
    "\n",
    "        loss_history_newton.append(loss.item())        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"Iteration {i}: loss = {loss.item():.7f}, min_loss_found_newton  = {min_loss_found_newton:.6f}\",)\n",
    "\n",
    "        if loss.item() < min_loss_found_newton:\n",
    "            min_loss_found_newton = loss.item()\n",
    "            best_height_params_newton = h_params.data.clone()\n",
    "    return best_height_params_newton, min_loss_found_newton, loss_history_newton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "height_params_newton = height_params.data.detach().clone() # initialize variable at value from test cell \n",
    "height_params_newton.requires_grad = True\n",
    "\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history_newton' not in locals():\n",
    "    loss_history_newton = []\n",
    "max_iter = 100000\n",
    "target_loss = 1.52\n",
    "print_every = 1\n",
    "\n",
    "if 'min_loss_found_newton' not in locals():\n",
    "    min_loss_found_newton = float('inf')\n",
    "\n",
    "if 'best_height_params_newton' not in locals():\n",
    "    best_height_params_newton = height_params_newton.data.clone()\n",
    "\n",
    "print(\"Starting Newton method\")\n",
    "best_height_params_newton, min_loss_found_newton, loss_history_newton = newton_method(height_params_newton, loss_fn, max_iter, target_loss, print_every, loss_history_newton, min_loss_found_newton, best_height_params_newton)\n",
    "\n",
    "print(\"Best loss found:\", min_loss_found_newton)\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history_newton)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Newton method: Loss History\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
