{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the step function, the integral, and the autoconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step function representation:\n",
    "# The step function f is implicitly defined by:\n",
    "# - heights: a 1D tensor of P non-negative values h_i.\n",
    "# - P: number of pieces.\n",
    "# - interval_range: a tuple (x_min, x_max), e.g., (-0.25, 0.25).\n",
    "# - delta_x: width of each piece, (x_max - x_min) / P.\n",
    "# - x_coords: tensor of starting x-coordinates for each piece.\n",
    "# For optimization, 'heights' would be your learnable parameters.\n",
    "#\n",
    "# Example of how you might define these:\n",
    "# P = 600\n",
    "# interval_range = (-0.25, 0.25)\n",
    "# heights = torch.rand(P, dtype=torch.float64, requires_grad=True) # example heights\n",
    "# delta_x = (interval_range[1] - interval_range[0]) / P\n",
    "# x_min = interval_range[0]\n",
    "\n",
    "def compute_integral_of_step_function(heights: torch.Tensor, delta_x: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the integral of the step function f(x)dx.\n",
    "    f(x) is defined by `heights` h_i over pieces of width `delta_x`.\n",
    "    Integral = delta_x * sum(h_i).\n",
    "    \"\"\"\n",
    "    if not torch.all(heights >= 0):\n",
    "        # print(\"warning: heights should be non-negative for the problem context.\")\n",
    "        # depending on your optimization strategy, you might enforce this elsewhere (e.g. h_i = relu(alpha_i))\n",
    "        pass # up to you how strict to be here, problem says \"non-negative f\"\n",
    "    return delta_x * torch.sum(heights)\n",
    "\n",
    "def compute_autoconvolution_values(heights: torch.Tensor, delta_x: float, P: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the values of the autoconvolution (f*f)(t) at the knot points.\n",
    "    (f*f)(t) is piecewise linear. Max value occurs at one of these knots.\n",
    "    Knots are t_m = 2*x_min + m*delta_x for m = 0, ..., 2P.\n",
    "    Values are [0, delta_x * (H*H)_0, ..., delta_x * (H*H)_{2P-2}, 0].\n",
    "    (H*H) is the discrete convolution of the height sequence H.\n",
    "    \"\"\"\n",
    "    # Ensure heights is 1D\n",
    "    if heights.ndim != 1 or heights.shape[0] != P:\n",
    "        raise ValueError(f\"heights tensor must be 1D with length P={P}. Got shape {heights.shape}\")\n",
    "\n",
    "    # Reshape heights for conv1d: (batch_size, C_in, L_in)\n",
    "    # batch_size=1, C_in=1\n",
    "    h_signal = heights.view(1, 1, P)\n",
    "    \n",
    "    # The kernel for conv1d to compute (H*H) should be H flipped.\n",
    "    # weight for conv1d: (C_out, C_in/groups, L_kernel)\n",
    "    # C_out=1, C_in/groups=1\n",
    "    h_kernel_flipped = torch.flip(heights, dims=[0]).view(1, 1, P)\n",
    "\n",
    "    # Compute H*H using conv1d. Padding P-1 results in output length 2P-1.\n",
    "    # These are (H*H)_0, ..., (H*H)_{2P-2}\n",
    "    conv_result = F.conv1d(h_signal, h_kernel_flipped, padding=P-1).squeeze()\n",
    "    \n",
    "    # Scale by delta_x\n",
    "    conv_scaled = delta_x * conv_result\n",
    "    \n",
    "    # Add zeros for (f*f)(t_0) and (f*f)(t_{2P})\n",
    "    zero = torch.tensor([0.0], device=heights.device, dtype=heights.dtype)\n",
    "    autoconvolution_knot_values = torch.cat([zero, conv_scaled, zero])\n",
    "    \n",
    "    return autoconvolution_knot_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the step function and the autoconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script assumes 'compute_integral_of_step_function' and 'compute_autoconvolution_values'\n",
    "# (from your previous code block) are already defined and executable in the notebook.\n",
    "\n",
    "#@title Imports and Plotting Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- plotting functions ---\n",
    "def plot_rendered_step_function(heights_numpy: np.ndarray, interval: tuple[float, float], title=\"\"):\n",
    "    \"\"\"plots a step function f(x) cleanly using plt.step.\"\"\"\n",
    "    P = len(heights_numpy)\n",
    "    x_min, x_max = interval\n",
    "    \n",
    "    step_edges = np.linspace(x_min, x_max, P + 1, dtype=float) # ensure float for plotting\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.step(step_edges[:-1], heights_numpy, where='post', color='blue', linewidth=1.5)\n",
    "    \n",
    "    plt.axhline(0, color='black', linewidth=0.5) # reference line at y=0\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    # dynamic axis limits\n",
    "    x_padding = 0.05 * (x_max - x_min) if (x_max - x_min) > 0 else 0.05\n",
    "    plt.xlim([x_min - x_padding, x_max + x_padding])\n",
    "    \n",
    "    if P > 0:\n",
    "        max_h = np.max(heights_numpy)\n",
    "        min_h = np.min(heights_numpy) # relevant if heights could be negative\n",
    "        if max_h > 0: # typical case for non-negative heights\n",
    "            plt.ylim([-0.1 * max_h, max_h * 1.2])\n",
    "        elif max_h == 0 and min_h == 0 : # all zero heights\n",
    "            plt.ylim([-0.5, 0.5])\n",
    "        else: # other cases (e.g. all negative, not expected here)\n",
    "            plt.ylim([min_h - 0.1*abs(min_h), max_h + 0.1*abs(max_h)])\n",
    "\n",
    "    else: # P=0, no data\n",
    "        plt.ylim([-0.5, 1.0]) \n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_rendered_convolution(t_values_numpy: np.ndarray, conv_values_numpy: np.ndarray, title=\"\"):\n",
    "    \"\"\"plots a piecewise linear function, e.g., the autoconvolution f*f(t).\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(t_values_numpy, conv_values_numpy, marker='o', linestyle='-', color='green', markersize=3, linewidth=1.5)\n",
    "    \n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"f*f(t)\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    if len(t_values_numpy) > 0:\n",
    "        t_min, t_max = t_values_numpy[0], t_values_numpy[-1]\n",
    "        t_padding = 0.05 * (t_max - t_min) if (t_max - t_min) > 0 else 0.05\n",
    "        plt.xlim([t_min - t_padding, t_max + t_padding])\n",
    "        \n",
    "        max_conv = np.max(conv_values_numpy)\n",
    "        min_conv = np.min(conv_values_numpy)\n",
    "        # autoconvolution of non-negative f(x) is non-negative\n",
    "        if max_conv > 0:\n",
    "            plt.ylim([-0.1 * max_conv, max_conv * 1.2])\n",
    "        else: # all zero (or P=0 for f(x))\n",
    "            plt.ylim([-0.5, 0.5])\n",
    "    else: # no data\n",
    "        plt.xlim([-0.55, 0.55]) # default based on expected autoconv range\n",
    "        plt.ylim([-0.1, 1.0])\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- main script logic ---\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "\n",
    "# 1. sample random heights (non-negative)\n",
    "# using float64 for precision, requires_grad=False for plotting/evaluation only\n",
    "height_values = torch.rand(P_val, dtype=torch.float64) \n",
    "# for more structured/sparse functions, you could try:\n",
    "# height_values = torch.abs(torch.randn(P_val, dtype=torch.float64)) * (torch.rand(P_val, dtype=torch.float64) > 0.7).double()\n",
    "\n",
    "\n",
    "# 2. compute integral of f(x)\n",
    "integral_value = compute_integral_of_step_function(height_values, f_delta_x)\n",
    "\n",
    "# 3. compute autoconvolution (f*f)(t) knot values\n",
    "# (2*P_val + 1) values for t_m from 2*f_x_min to 2*f_x_max\n",
    "autoconv_knot_vals = compute_autoconvolution_values(height_values, f_delta_x, P_val)\n",
    "max_autoconv_value = torch.max(autoconv_knot_vals)\n",
    "\n",
    "# 4. calculate the ratio for C1 estimate\n",
    "# (max f*f(t)) / (integral f(x) dx)^2\n",
    "if integral_value.item() == 0:\n",
    "    c1_ratio = float('inf') if max_autoconv_value.item() > 0 else 0.0 # handle division by zero\n",
    "else:\n",
    "    c1_ratio = max_autoconv_value / (integral_value**2)\n",
    "\n",
    "print(f\"--- Function Details (P={P_val}) ---\")\n",
    "# print(f\"Heights (h_i, first 10): {height_values.numpy()[:10]}\") # uncomment if you want to see heights\n",
    "print(f\"Integral of f(x): {integral_value.item():.6f}\")\n",
    "print(f\"Max value of autoconvolution (f*f)(t): {max_autoconv_value.item():.6f}\")\n",
    "print(f\"Ratio max(f*f) / (integral(f))^2: {c1_ratio.item():.6f}\")\n",
    "\n",
    "\n",
    "# 5. plotting\n",
    "# plot f(x)\n",
    "plot_rendered_step_function(height_values.numpy(), f_interval, title=f\"Random Step Function f(x) (P={P_val})\")\n",
    "\n",
    "# plot f*f(t)\n",
    "# autoconvolution is defined on [2*f_x_min, 2*f_x_max]\n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals.numpy(), title=f\"Autoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection onto the simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def projection_simplex_pytorch(v: torch.Tensor, z: float = 1.0) -> torch.Tensor:\n",
    "    n_features = v.shape[0]\n",
    "    if n_features == 0:\n",
    "        return torch.empty_like(v)\n",
    "    u, _ = torch.sort(v, descending=True)\n",
    "    cssv_minus_z = torch.cumsum(u, dim=0) - z\n",
    "    ind = torch.arange(1, n_features + 1, device=v.device) \n",
    "    cond = u - cssv_minus_z / ind > 0\n",
    "    true_indices = torch.where(cond)[0]\n",
    "    rho_idx = true_indices[-1] \n",
    "    rho = ind[rho_idx] \n",
    "    theta = cssv_minus_z[rho_idx] / rho \n",
    "    w = torch.clamp(v - theta, min=0.0)\n",
    "    return w\n",
    "\n",
    "# test the function\n",
    "v = torch.tensor([1.0, 2.0, 3.0])\n",
    "z = 100\n",
    "w = projection_simplex_pytorch(v, z)\n",
    "print(\"w\", w, \"sum(w)\", sum(w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parameters, loss function, and compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting function parameters\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "\n",
    "# Define optimization variable\n",
    "height_params = torch.rand(P_val, dtype=torch.float64)\n",
    "height_params.requires_grad = True\n",
    "\n",
    "def loss_fn(): \n",
    "    max_f_conv_f_values_at_knots = compute_autoconvolution_values(height_params, f_delta_x, P_val)\n",
    "    return max_f_conv_f_values_at_knots.max()\n",
    "\n",
    "# Project variables onto the simplex \n",
    "with torch.no_grad():\n",
    "    height_params.data = projection_simplex_pytorch(height_params.data, 2*P_val)\n",
    "\n",
    "# Compute loss and gradient\n",
    "height_params.grad = None\n",
    "loss = loss_fn()\n",
    "loss.backward()\n",
    "print(\"loss\", loss.item())\n",
    "\n",
    "print(\"height_params.grad\", height_params.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: (Projected) Polyak subgradient method\n",
    "\n",
    "The algorithm \n",
    "$$\n",
    "h_+ = \\mathrm{proj}_{\\Delta_{2P}}\\left(h - \\frac{L(h) - L^*}{\\| \\nabla L(h) \\|^2} \\nabla L(h)\\right)\n",
    "$$\n",
    "where $L^*$ is the target loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100000\n",
    "height_params_polyak = height_params.data.clone()\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history_polyak' not in locals():\n",
    "    loss_history_polyak = []\n",
    "\n",
    "target_loss = 1.5179\n",
    "print_every = 100\n",
    "if 'min_loss_found_polyak' not in locals():\n",
    "    min_loss_found_polyak = float('inf')\n",
    "\n",
    "if 'best_height_params_polyak' not in locals():\n",
    "    best_height_params_polyak = height_params.data.clone()\n",
    "\n",
    "print(\"Starting Polyak subgradient method\")\n",
    "for i in range(max_iter):\n",
    "    # Compute loss and gradient\n",
    "    height_params_polyak.grad = None\n",
    "    loss = loss_fn()\n",
    "    loss.backward()\n",
    "\n",
    "    grad = height_params_polyak.grad.data\n",
    "    grad_norm_squared = torch.norm(grad)**2\n",
    "    \n",
    "    step_size = (loss.item() - target_loss) / grad_norm_squared\n",
    "    height_params_polyak.data -= step_size * grad\n",
    "\n",
    "    # Project onto the simplex\n",
    "    with torch.no_grad():\n",
    "        height_params_polyak.data = projection_simplex_pytorch(height_params_polyak.data, 2*P_val)\n",
    "\n",
    "    loss_history_polyak.append(loss.item())        \n",
    "    if i % print_every == 0:\n",
    "        print(f\"Iteration {i}: loss = {loss.item():.6f}, step_size = {step_size:.6f}, min_loss_found_polyak = {min_loss_found_polyak:.6f}\",)\n",
    "\n",
    "    if loss.item() < min_loss_found_polyak:\n",
    "        min_loss_found_polyak = loss.item()\n",
    "        best_height_params_polyak = height_params_polyak.data.clone()\n",
    "\n",
    "print(\"Final loss:\", loss.item())\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: (Projected) perturbed subgradient method\n",
    "\n",
    "The algorithm\n",
    "$$\n",
    "h_+ = \\mathrm{proj}_{\\Delta_{2P}}\\left(h - \\gamma_k (\\nabla L(h) + \\xi_k)\\right)\n",
    "$$\n",
    "where $\\xi_k$ is a random vector with mean 0 and variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100000\n",
    "# if loss history exists, append to it\n",
    "if 'loss_history' not in locals():\n",
    "    loss_history = []\n",
    "\n",
    "print_every = 100\n",
    "if 'min_loss_found' not in locals():\n",
    "    min_loss_found = float('inf')\n",
    "\n",
    "if 'best_height_params' not in locals():\n",
    "    best_height_params = height_params.data.clone()\n",
    "\n",
    "height_params.data = best_height_params.data.detach()\n",
    "sigma = 0.00001\n",
    "for i in range(max_iter):\n",
    "    # Compute loss and gradient\n",
    "    height_params.grad = None\n",
    "    loss = loss_fn()\n",
    "    loss.backward()\n",
    "\n",
    "    grad = height_params.grad.data\n",
    "    grad_norm_squared = torch.norm(grad)**2\n",
    "    step_size = 100/(torch.tensor(i+1, dtype=torch.float64))\n",
    "    height_params.data -= step_size * (grad + torch.randn_like(grad) * sigma)\n",
    "\n",
    "    # Project onto the simplex\n",
    "    with torch.no_grad():\n",
    "        height_params.data = projection_simplex_pytorch(height_params.data, 2*P_val)\n",
    "\n",
    "    loss_history.append(loss.item())        \n",
    "    if i % print_every == 0:\n",
    "        print(f\"Iteration {i}: loss = {loss.item():.6f}, step_size = {step_size:.6f}, min_loss_found = {min_loss_found:.6f}\",)\n",
    "\n",
    "    if loss.item() < min_loss_found:\n",
    "        min_loss_found = loss.item()\n",
    "        best_height_params = height_params.data.clone()\n",
    "\n",
    "print(\"Final loss:\", loss.item())\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
