{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B1: First autocorrelation inequality\n",
    "For any function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$, define the *autoconvolution* of $f$, written $f*f$, as\n",
    "$$f*f (t) := \\int_\\mathbb{R} f(t-x) f(x)\\ dx.$$\n",
    "\n",
    "Let $C_1$ denote the largest constant for which one has\n",
    "\\begin{equation}\n",
    " \\max_{-1/2 \\leq t \\leq 1/2} f*f(t) \\geq C_1 \\left(\\int_{-1/4}^{1/4} f(x)\\ dx\\right)^2\n",
    "\\end{equation}\n",
    "for all non-negative $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.  This problem arises in additive combinatorics, relating to the size of Sidon sets.  It is currently known that\n",
    "$$ 1.28 \\leq C_1 \\leq 1.5098$$\n",
    "with the lower bound proven by [Cloninger and Steinerberger (2017)](https://www.ams.org/journals/proc/2017-145-08/S0002-9939-2017-13690-9/S0002-9939-2017-13690-9.pdf) and the upper bound achieved by [Matolcsi and Vinuesa (2010)](https://www.sciencedirect.com/science/article/pii/S0022247X10006001) via a step function construction. AlphaEvolve found a step function with 600 equally-spaced intervals on $[-1/4,1/4]$ that gives a better upper bound of $C_1 \\leq 1.5053$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the step function, the integral, and the autoconvolution\n",
    "\n",
    "For step func. \n",
    "$$f(x) = \\sum_{i=0}^{P-1} h_i \\mathbf{1}_{[x_i, x_{i+1})}(x)$$  \n",
    "where\n",
    "$$ x_i = -\\frac{1}{4} + \\frac{i}{2P}, \\text{ and heights } h_i \\ge 0:$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "% Integral\n",
    "\\text{Integral:}\\quad \\int_{-1/4}^{1/4} f(x) \\,dx &= \\frac{1}{2P} \\sum_{i=0}^{P-1} h_i\n",
    "\\\\[2ex]\n",
    "% Autoconvolution\n",
    "\\text{Autoconv. knots:}\\quad (f*f)(t_m) &= \n",
    "\\begin{cases} \n",
    "0 & \\text{if } m=0 \\text{ or } m=2P \\\\\n",
    "\\frac{1}{2P} \\sum_{k=\\max(0, m-P)}^{\\min(P-1, m-1)} h_k h_{m-1-k} & \\text{if } 1 \\le m \\le 2P-1\n",
    "\\end{cases}\n",
    "\\\\[0.5ex]\n",
    "&\\quad \\text{where } t_m = -\\frac{1}{2} + \\frac{m}{2P} \\quad (m=0, \\dots, 2P).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of the optimization problem\n",
    "\n",
    "Let $f_h$ denote the step function with heights $h$. We're trying to optimize the function:\n",
    "\\begin{align*}\n",
    "\\mathrm{minimize} &\\; L(h) := \\max_{-1/2\\leq t \\leq 1/2} (f_h\\ast f_h)(t) \\\\\n",
    "\\text{subject to } &\\; \\int_{-1/4}^{1/4} f_h(t) dt = \\frac{\\mathbf{1}^T h}{2P} = 1 \\\\\n",
    "&\\; h \\geq 0.\n",
    "\\end{align*}\n",
    "This is a nonsmooth qudaratic optimization problem over a simplex.\n",
    "\n",
    "Note that this is not convex: set $P=2$ and $h_0+h_1=2P=4$. The objective is\n",
    "$$\n",
    "\\frac{1}{2P}\\max(h_0^2, 2h_0h_1, h_1^2)\n",
    "$$\n",
    "We can parameterize the feasible region by $h_0 \\in [0,4]$. Then there is a region where $2h_0h_1 = 2h_0(4-h_0)$ attains the max and the function becomes concave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the step function and the autoconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script assumes 'compute_integral_of_step_function' and 'compute_autoconvolution_values'\n",
    "# (from your previous code block) are already defined and executable in the notebook.\n",
    "from utils import *\n",
    "\n",
    "# --- main script logic ---\n",
    "P_val = 600  # number of pieces for the step function\n",
    "f_interval = (-0.25, 0.25) # interval for f(x)\n",
    "\n",
    "f_x_min, f_x_max = f_interval\n",
    "f_delta_x = (f_x_max - f_x_min) / P_val\n",
    "\n",
    "# 1. sample random heights (non-negative)\n",
    "# using float64 for precision, requires_grad=False for plotting/evaluation only\n",
    "height_values = torch.rand(P_val, dtype=torch.float64) \n",
    "# for more structured/sparse functions, you could try:\n",
    "# height_values = torch.abs(torch.randn(P_val, dtype=torch.float64)) * (torch.rand(P_val, dtype=torch.float64) > 0.7).double()\n",
    "\n",
    "\n",
    "# 2. compute integral of f(x)\n",
    "integral_value = compute_integral_of_step_function(height_values, f_delta_x)\n",
    "\n",
    "# 3. compute autoconvolution (f*f)(t) knot values\n",
    "# (2*P_val + 1) values for t_m from 2*f_x_min to 2*f_x_max\n",
    "autoconv_knot_vals = compute_autoconvolution_values(height_values, f_delta_x, P_val)\n",
    "max_autoconv_value = torch.max(autoconv_knot_vals)\n",
    "\n",
    "# 4. calculate the ratio for C1 estimate\n",
    "# (max f*f(t)) / (integral f(x) dx)^2\n",
    "if integral_value.item() == 0:\n",
    "    c1_ratio = float('inf') if max_autoconv_value.item() > 0 else 0.0 # handle division by zero\n",
    "else:\n",
    "    c1_ratio = max_autoconv_value / (integral_value**2)\n",
    "\n",
    "print(f\"--- Function Details (P={P_val}) ---\")\n",
    "# print(f\"Heights (h_i, first 10): {height_values.numpy()[:10]}\") # uncomment if you want to see heights\n",
    "print(f\"Integral of f(x): {integral_value.item():.6f}\")\n",
    "print(f\"Max value of autoconvolution (f*f)(t): {max_autoconv_value.item():.6f}\")\n",
    "print(f\"Ratio max(f*f) / (integral(f))^2: {c1_ratio.item():.6f}\")\n",
    "\n",
    "\n",
    "# 5. plotting\n",
    "# plot f(x)\n",
    "plot_rendered_step_function(height_values.numpy(), f_interval, title=f\"Random Step Function f(x) (P={P_val})\")\n",
    "\n",
    "# plot f*f(t)\n",
    "# autoconvolution is defined on [2*f_x_min, 2*f_x_max]\n",
    "conv_t_knots = np.linspace(2 * f_x_min, 2 * f_x_max, 2 * P_val + 1, dtype=float)\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals.numpy(), title=f\"Autoconvolution f*f(t) (P={P_val})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Google's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "h_params_google = return_height_params_google_simplex()\n",
    "autoconv_knot_vals_google = compute_autoconvolution_values(h_params_google, f_delta_x, P_val)\n",
    "max_autoconv_value_google = torch.max(autoconv_knot_vals_google)\n",
    "\n",
    "plot_rendered_step_function(h_params_google.detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google.detach().numpy(), title=f\"Google'sAutoconvolution f*f(t) (P={P_val})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: (Projected) Polyak subgradient method\n",
    "\n",
    "The algorithm \n",
    "$$\n",
    "h_+ = \\mathrm{proj}_{\\Delta_{2P}}\\left(h - \\frac{L(h) - L^*}{\\| \\nabla L(h) \\|^2} \\nabla L(h)\\right)\n",
    "$$\n",
    "where $L^*$ is the target loss.\n",
    "\n",
    "There is a subtlety here: we don't know what the optimal value is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak with Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "# Initialize random height parameters\n",
    "h_params_polyak = torch.rand(P_val, dtype=torch.float64)\n",
    "h_params_polyak.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak.data = projection(h_params_polyak.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "# Setting up history\n",
    "if 'history_polyak_constrained_random' not in locals():\n",
    "    history_polyak_constrained_random = {}\n",
    "else:\n",
    "    h_params_polyak.data = history_polyak_constrained_random['best_h_params'].data.clone()\n",
    "\n",
    "# Setting up parameters for Polyak\n",
    "max_iter = 1000\n",
    "target_loss = 1.505\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_constrained_random = polyak_subgradient_method(h_params_polyak,loss_fn, max_iter, target_loss, print_every, history_polyak_constrained_random, projection)\n",
    "print(\"Best loss found:\", history_polyak_constrained_random['min_loss_found'])\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_constrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoconv_knot_vals_polyak = compute_autoconvolution_values(history_polyak_constrained_random['best_h_params'], f_delta_x, P_val)\n",
    "max_autoconv_value_polyak = torch.max(autoconv_knot_vals_polyak)\n",
    "\n",
    "plot_rendered_step_function(history_polyak_constrained_random['best_h_params'].detach().numpy(), f_interval, title=f\"Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_polyak.detach().numpy(), title=f\"Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak started at the Google solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "h_params_polyak_google = h_params_google.data.detach().clone() # initialize variable at google's solution.\n",
    "h_params_polyak_google.requires_grad = True\n",
    "# Define the projection and loss\n",
    "def projection(x): \n",
    "    return projection_simplex_pytorch(x, 2*P_val)\n",
    "h_params_polyak_google.data = projection(h_params_polyak_google.data)\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "# Setting up history\n",
    "if 'history_polyak_constrained_google' not in locals():\n",
    "    history_polyak_constrained_google = {}\n",
    "else:\n",
    "    h_params_polyak_google.data = history_polyak_constrained_google['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 1000\n",
    "target_loss_google = 1.505292\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_constrained_google = polyak_subgradient_method(h_params_polyak_google, loss_fn, max_iter, target_loss_google, print_every, history_polyak_constrained_google, projection)\n",
    "\n",
    "print(\"Best loss found (Google solution initialization):\", history_polyak_constrained_google['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_constrained_google['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method (Google solution initialization): Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot google's solution\n",
    "autoconv_knot_vals_google_polyak = compute_autoconvolution_values(history_polyak_constrained_google['best_h_params'], f_delta_x, P_val)\n",
    "max_autoconv_value_google_polyak = torch.max(autoconv_knot_vals_google_polyak)\n",
    "\n",
    "plot_rendered_step_function(history_polyak_constrained_google['best_h_params'].detach().numpy(), f_interval, title=f\"Google's Step Function f(x) (After Polyak) (P={P_val})\")\n",
    "plot_rendered_convolution(conv_t_knots, autoconv_knot_vals_google_polyak.detach().numpy(), title=f\"Google's Autoconvolution f*f(t) (After Polyak) (P={P_val})\")\n",
    "\n",
    "torch.set_printoptions(precision=16, sci_mode=False)  # global setting\n",
    "\n",
    "print(\"best_height_params_polyak_google\", history_polyak_constrained_google['best_h_params'])\n",
    "# need 16 digits of precision for the loss for the print statement to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the loss function to remove constraints\n",
    "\n",
    "We're going to precompose the loss function with a softmax. This implicitly constrains the functions to the simplex. I.e., we will optimize\n",
    "\n",
    "$$\n",
    "L(2P\\mathrm{softmax}(w))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyak on unconstrained (softmax) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "P_val = 600\n",
    "h_params_polyak_unconstrained = torch.rand(P_val, dtype=torch.float64)\n",
    "h_params_polyak_unconstrained.requires_grad = True\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "\n",
    "if 'history_polyak_unconstrained_random' not in locals():\n",
    "    history_polyak_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_polyak.data = history_polyak_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 100\n",
    "target_loss = 1.52\n",
    "print_every = 1000\n",
    "\n",
    "history_polyak_unconstrained_random = polyak_subgradient_method(h_params_polyak_unconstrained, loss_f_softmax, max_iter, target_loss, print_every, history_polyak_unconstrained_random)\n",
    "\n",
    "print(\"Best loss found:\", history_polyak_unconstrained_random['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_polyak_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_polyak_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Polyak subgradient method: Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS on unconstrained loss (with weak wolfe line search)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "h_params_bfgs_unconstrained = (torch.randn(P_val).abs() * 0.01 + 0.01).log()\n",
    "# h_params_bfgs_unconstrained = h_params_google # Start from google's solution if you want.\n",
    "# h_params_bfgs_unconstrained.data = torch.log(h_params_bfgs_unconstrained.data)\n",
    "h_params_bfgs_unconstrained.requires_grad = True\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "\n",
    "if 'history_bfgs_unconstrained_random' not in locals():\n",
    "    history_bfgs_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_bfgs_unconstrained.data = history_bfgs_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 1\n",
    "bfgs_params = {}\n",
    "bfgs_params['lr'] = 1\n",
    "bfgs_params['history_size'] = 2000\n",
    "bfgs_params['ls_params'] = {'c1': 0.0001, 'c2': 0.9}\n",
    "# bfgs_params['line_search_fn'] = None # no line search function. Will likely blow up.\n",
    "bfgs_params['line_search_fn'] = 'weak_wolfe' # good for later iterations\n",
    "print_every = 100\n",
    "\n",
    "history_bfgs_unconstrained_random = bfgs_method(h_params_bfgs_unconstrained, loss_f_softmax, max_iter, bfgs_params, print_every, history_bfgs_unconstrained_random)\n",
    "\n",
    "print(\"Best loss found:\", history_bfgs_unconstrained_random['min_loss_found'])\n",
    "\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_bfgs_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"BFGS method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_bfgs_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"BFGS method: Gradient Norm History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTD on unconstrained loss \n",
    "\n",
    "From https://github.com/COR-OPT/ntd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_val = 600\n",
    "h_params_ntd_unconstrained = torch.rand(P_val, dtype=torch.float64)\n",
    "# h_params_ntd_unconstrained = h_params_google # Start from google's solution if you want.\n",
    "h_params_ntd_unconstrained.data = torch.log(h_params_ntd_unconstrained.data)\n",
    "h_params_ntd_unconstrained.requires_grad = True\n",
    "\n",
    "loss_f_softmax = unconstrained_loss(P_val)\n",
    "\n",
    "if 'history_ntd_unconstrained_random' not in locals():\n",
    "    history_ntd_unconstrained_random = {}\n",
    "else:\n",
    "    h_params_ntd_unconstrained.data = history_ntd_unconstrained_random['best_h_params'].data.clone()\n",
    "\n",
    "max_iter = 1\n",
    "ntd_params = {}\n",
    "ntd_params['adaptive_grid_size'] = True\n",
    "ntd_params['verbose'] = False\n",
    "ntd_params['max_grid_size'] = 15 # 50 \n",
    "# ntd_params['opt_f'] = 1.5052\n",
    "ntd_params['min_goldstein_iters'] = 1\n",
    "ntd_params['use_trust_region'] = True\n",
    "print_every = 1\n",
    "\n",
    "history_ntd_unconstrained_random = ntd_method(h_params_ntd_unconstrained, loss_f_softmax, max_iter, ntd_params, print_every, history_ntd_unconstrained_random)\n",
    "\n",
    "print(\"Best loss found:\", history_ntd_unconstrained_random['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_ntd_unconstrained_random['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"NTD method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_ntd_unconstrained_random['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"NTD method: Gradient Norm History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prox linear method \n",
    "\n",
    "Basic idea: The objective function is a max of smooth function.\n",
    "\n",
    "$$\n",
    "L(h) = \\max_{i} \\ell_i(h)\n",
    "$$\n",
    "\n",
    "So we linearize them smooth function and then minimize the linearization over the constraint. In particular, given current iterate $h$, we solve:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_+ = &\\mathrm{argmin}_{h'} \\max_{i} \\{ \\ell_i(h) + \\nabla \\ell_i(h) (h' - h) \\} + \\frac{\\gamma}{2} \\|h' - h\\|^2\\\\\n",
    "&\\text{subject to }  h' \\in \\Delta_{2P}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: \n",
    "- For large enough $\\gamma$ (bigger than maximal lipschitz constant of gradient of $\\ell_i$), each step is guaranteed to reduce the loss (if not at critical point), but progress is slower.\n",
    "- For $\\gamma = 0$, it takes bigs steps, good for initial iterates, but can start to cycle. \n",
    "- Lowest sol'n so far found is 1.5160005027, and i'm still running the method. It uses 1000 heights instead of 600.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "P_val = 1000\n",
    "h_params_prox_linear = torch.rand(P_val, dtype=torch.float64)\n",
    "# h_params_prox_linear = h_params_google.detach().clone() # Start from google's solution if you want.\n",
    "h_params_prox_linear.data = 1200*h_params_prox_linear.data/sum(h_params_prox_linear.data)\n",
    "h_params_prox_linear.requires_grad = True\n",
    "loss_fn = constrained_loss(P_val)\n",
    "\n",
    "file_path = 'best_found_so_far_prox_linear.pt'\n",
    "if os.path.exists(file_path):\n",
    "    saved_best_h_params = torch.load(file_path)\n",
    "    if 'history_prox_linear' not in locals():\n",
    "        history_prox_linear = {}\n",
    "        history_prox_linear['best_h_params'] = saved_best_h_params\n",
    "        h_params_prox_linear.data = saved_best_h_params.data.clone()\n",
    "    else:\n",
    "        if loss_fn(saved_best_h_params) > (loss_fn(history_prox_linear['best_h_params'])):\n",
    "            torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "            h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "            print(\"Current point better than point in file. Saving current point.\")\n",
    "        else:\n",
    "            h_params_prox_linear.data = saved_best_h_params.data.clone()\n",
    "            print(\"Point in file better than current point. Loading point in file.\")\n",
    "else:\n",
    "    if 'history_prox_linear' not in locals():\n",
    "        history_prox_linear = {}\n",
    "    else: \n",
    "        torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "        h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "        print(\"No point in file. Saving current point.\")\n",
    "\n",
    "max_iter = 30 # try to run around 10-30 iterations at a time. can always pause and resume.\n",
    "gamma_prox = 0 # initially set to zero, but then try .00001, .0001, .001, etc. set as low as possible while you still make progress.\n",
    "print_every = 1\n",
    "# Solver parameters. Seems like CLARABEL is better for this problem.\n",
    "## For ECOS, Set error tolerances around 1e-9, then gradually decrease.\n",
    "# prox_linear_params = {'solver' : 'ECOS', 'gamma_prox': gamma_prox, 'abstol': 1e-10, 'reltol': 1e-09, 'feastol': 1e-09, 'max_iters': 2000, 'verbose': False}\n",
    "## Clarabel can handle much lower tolerances.\n",
    "tol = 1e-17\n",
    "prox_linear_params = {'solver' : 'CLARABEL', 'gamma_prox': gamma_prox, 'abstol': tol, 'reltol': tol, 'feastol': tol, 'max_iters': 2000, 'verbose': False}\n",
    "history_prox_linear = prox_linear_method(h_params_prox_linear, loss_fn, print_every, max_iter, prox_linear_params, history_prox_linear)\n",
    "\n",
    "print(\"Best loss found:\", history_prox_linear['min_loss_found'])\n",
    "\n",
    "# semilogy plot of loss history\n",
    "plt.semilogy(history_prox_linear['loss_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Proximal linear method: Loss History\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(history_prox_linear['grad_norm_history'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Grad norm history\")\n",
    "plt.title(\"Proximal linear method: Gradient Norm History\")\n",
    "plt.show()\n",
    "\n",
    "file_path = 'best_found_so_far_prox_linear.pt'\n",
    "torch.save(history_prox_linear['best_h_params'], file_path)\n",
    "h_params_prox_linear.data = history_prox_linear['best_h_params'].data.clone()\n",
    "print(\"No point in file. Saving current point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP solver from [Matolcsi and Vinuesa (2010)](https://www.sciencedirect.com/science/article/pii/S0022247X10006001)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
